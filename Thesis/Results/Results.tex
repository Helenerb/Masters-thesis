\newpage
\section{Results}
%First, we use synthetic data to show that $\texttt{inlabru}$ produces restults that are sufficiently similar to the one produced by more traditional MCMC approaches. For a fair comparison between the $\texttt{INLA}$ framework and state-of-the-art MCMC methods, we compare results of Bayesian inference using $\texttt{inlabru}$ to results using inference from the STAN methodology. The STAN methodology is an implementation of a Hamiltonian Monte Carlo approach, and is described more closely in Section \ref{sec:stan}. We use the $\texttt{R}$-library $\texttt{rstan}$ to do inference with STAN. 

\subsection{STAN: Hamiltonian Monte Carlo}
\label{sec:stan}
%STAN is a method for Bayesian inference using a Hamiltonian Monte Carlo approach. It was proposed by ... and has since its introduction gained popularity for its computational power compared to traditional MCMC approaches. bla bla ikke ferdig!!!

\import{}{Results/GermanCancerData}

\newpage
\subsection{Our Model Definition}
For the analysis in this investigation, we will consider a version of the Poisson-extended Lee-Carter model, with an extra error term accounting for overdispersion \textcolor{myDarkGreen}{Sjekk hva dette egentlig betyr}. We will consider this model with and without an additional cohort effect. We note that we do not include a modulating age factor for the cohort effect, as suggested by e.g. \textcite{Renshaw_Haberman_2009} \textcolor{myDarkGreen}{Finn ut hvilke andre, hvem var det som først foreslo det? }, so that we do not overcomplicate the model, as it was argued by \textcolor{myDarkGreen}{FINN UT HVEM} that it might. We refer to our version of the Lee-Carter model without a cohort effect as the LC model, and to the version for which we include a cohort effect as the LCC model. They are defined as follows:
\begin{align}
    \text{LC:}&\quad Y_{x,t} \sim \Poisson(E_{x,t}e^{\eta_{xt}}), \quad \eta_{x,t} = \mu + \alpha_x + \beta_x\kappa_t + \epsilon_{x,t} \label{eq:LC-model}\\
    \text{LCC:}&\quad Y_{x,t} \sim \Poisson(E_{x,t}e^{\eta_{xt}}), \quad \eta_{x,t} = \mu + \alpha_x + \beta_x\kappa_t + \gamma_{c} + \epsilon_{x,t}. \label{eq:LCC-model}
\end{align}
Here, $Y_{x,t}$ are the number of deaths of the population of age $x$ during period $t$ and $E_{x,t}$ are the corresponding initial exposed population to the risk of death. $\alpha_x$, $\beta_x$, $\kappa_t$ and $\gamma_c$ are the age-, modulating age-, period- and cohort effects, respectively. This model represents the stochasticity of the affected cases $Y_{x,t}$ of a population $E_{x,t}$ in two ways. Firstly, through the random term $\epsilon_{x,t}$, which describes variability in the expected mortality rate that cannot be explained by age, period or cohort effects. Secondly, the Poisson distribution describes the randomness of the realized cases $Y_{x,t}$ given the expected mortality rate $E_{x,y}e^{\eta_{x,t}}$. When accounting for overdispersion in this way, we follow FINN UT HVEM, who argues that simply relying on the variability of the Poisson distribution, which is bounded by the size of the mean, to account for the variability in observed mortality is insufficient. \textcolor{myDarkGreen}{Skriv dette litt bedre når du har skrevet om dette i teori}

To ensure identifiability, we constrain the age-modulating effect $\beta_x$, the period effect $\kappa_t$ \textcolor{myDarkGreen}{and the cohort effect $\gamma_c$} by
\begin{equation*}
    \sum_x\beta_x = 1, \quad \sum_t\kappa_t = 0, \textcolor{myDarkGreen}{\sum_c\gamma_c = 0}.
\end{equation*}

%$\epsilon_{x,t}$ is an error term, describing variability in the data that cannot be explained by either any of the time effects or by the variance of the Poisson distribution. The additional error term $\epsilon_{x,t}$ 
We note that in contrast to e.g. the classical Lee-Carter formulation, we add an intercept $\mu$ to the model structure. We do this to extract a separate value for the overall mortality level, and by enforcing $\sum_x \alpha_x = 0$, this is equivalent to models without an intercept and where the values of $\alpha_x$ are unconstrained. Consequently, our full set of constraints becomes
\begin{equation}
    \sum_x\lapha_x = 0,\quad  \sum_x\beta_x = 1, \quad \sum_t\kappa_t = 0, \textcolor{myDarkGreen}{\sum_c\gamma_c = 0}.
    \label{eq:LC-constraints}
\end{equation}

It is common to denote the index of the cohort effects $c$ simply by $c = t-x$, since this is indeed the relationship between the age, period and cohort indices for data with equally spaced age- and period-intervals. We refrain from doing this, and keep the general cohort index $c$, to allow for data with unequal time intervals, e.g. data that is measured for period intervals of one year and age intervals of five years. 

\subsection{Bayesian Inference with the Lee-Carter Models}
\label{sec:BayesianInferenceLC}
\textcolor{myDarkRed}{CHECK CORRECTNESS - FROM PROJECT}
We consider the Lee-Carter model in a Bayesian setting, which means that we treat the effects $\alpha_x$, $\beta_x$, $\kappa_t$ and $\gamma_k$ as unknown random effects, with a likelihood model given by Equation \ref{eq:LC-model} or \ref{eq:LCC-model} and with some prior distributions. As discussed in Section \ref{section:Lee-Carter}, a random walk with drift is normally used to model the period effect $\kappa_t$, and so this is the natural choice for the prior distribution for $\kappa_t$. However, keeping in mind that our goal is to consider this model in the \inla framework, which does not support random walk with drifts, we instead choose a random walk of order two (\rwtwo) for the period effect:
\begin{equation*}
    \Delta^2\kappa_t = \kappa_t - 2\kappa_{t + 1} + \kappa_{t + 2} \sim \Normal(0, 1/\tau_{\kappa})
\end{equation*}
Here, $\tau_{\kappa}$ is the precision of the normal distribution. An \rwtwoS model is normally distributed around a line, and so we expect it to capture the linear trends of the period effect well \textcolor{myBluePurple}{Sara: is this correct?}. 
We assign driftless random walks as priors to $\alpha_t$ and $\gamma_k$:
\begin{equation*}
    \begin{aligned}
        \Delta\alpha_x = \alpha_x - \alpha_{x-1} \sim \Normal(0, 1/\tau_{\alpha}) \\
        \Delta\gamma_c = \gamma_c - \gamma_{c-1} \sim \Normal(0, 1/\tau_{\gamma}),
    \end{aligned}
\end{equation*}
where $\tau_{\alpha}$ and $\tau_{\gamma}$ are the precisions of the random terms $\epsilon_x$ and $\epsilon_k$, respectively, of the two random walks. Taking into consideration that $\beta_x$ often takes a less smooth shape than the other effects (see Section \ref{section:Lee-Carter}), we assign an \iidS Gaussian prior to $\beta_x$:
\begin{equation}
    \beta_x \sim \Normal(0, 1/\tau_{\beta}).
\end{equation}
Since $\epsilon_{x,t}$ is included to model randomness in the output, it is natural to choose an \iidS Gaussian prior for this effect as well:
\begin{equation}
    \epsilon_{x,t} &\sim \Normal(0, 1/\tau_{\epsilon}).
\end{equation}
In the two expressions above, $\tau_\beta$ and $\tau_\epsilon$ are the precisions of the respective Gaussian distributions. We note that our choice of keeping $\tau_\epsilon$ common for all $\epsilon_{x,t}$ we assume an heteroscedastic error structure. This assumption has been argued by \textcolor{myDarkGreen}{HVEM? Hunt & Blake tror jeg, noen andre} to be faulty, which we keep in mind. However, we make this choice to keep the model simple, and since the error structure is not our point of focus. \textcolor{myDarkGreen}{Ikke helt sikker på den setningen her, men tror det er smart å nevne valget}

\textcolor{myDarkGreen}{This is not correct - find out where you should talk about choices of priors - you do want to use PC-priors when you apply the model to real data, but not when you compare with STAN}
As for the APC model, we assign PC priors to the hyperparameters $\tau_\kappa, \tau_\alpha, \tau_\gamma, \tau_\beta, \tau_\epsilon$.

\subsection{Using the \inla Framework for Inference with Lee-Carter Models}
\newpar We now investigate how to apply the \inla method for Bayesian inference to the Lee-Carter models. We begin by showing that the Lee-Carter models can be written as LGMs. We write the model in Equation \ref{eq:LC-model} as 
\begin{equation}
    \textbf{y}\mid \textbf{x}, \boldsymbol{\theta} \sim \prod_{x,t}\Poisson(E_{x,t}e^{\eta_{x,t}}\mid \boldsymbol{\theta})
\end{equation}
where $\eta_{x,t}$ is given uniquely by $\{\alpha_x, \beta_x, \kappa_t, \epsilon_{x,t}\}$ as in Equation \ref{eq:LC-model} and $\Vector{\theta}$ is the vector of hyperparameters, $\Vector{\theta} = \{\tau_{\eta}, \tau_{\alpha}, \tau_{\beta}, \tau_{\kappa}, \tau_\epsilon\}$. The latent field $\Vector{x}$ can then be represented by 
\begin{equation*}
    \Vector{x} = (\alpha_1,\ldots,\alpha_X,\beta_1,\ldots,\beta_X, \kappa_1,\ldots, \kappa_T, \epsilon_1, \ldots, \epsilon_N)
\end{equation*}
or equivalently 
\begin{equation}
    \Vector{x} = (\eta_1,\ldots,\eta_N,\alpha_1,\ldots,\alpha_X,\beta_1,\ldots,\beta_X, \kappa_1,\ldots, \kappa_T)
    \label{eq:LChyperparams}
\end{equation}
when $x\in \{1,\ldots,X\}$, $t\in\{1,\ldots,T\}$ and $N=X\times T$. Furthermore, we can now say that the latent field $\Vector{x}$ follows a normal distribution given $\Vector{\theta}$:
\begin{equation}
        \Vector{x}\mid \Vector{\theta} \sim \Normal(\Vector{0}, \Matrix{Q}^{-1}(\Vector{\theta})),
        \label{eq:XNormalDist}
\end{equation}
where $\Matrix{Q}^{-1}(\Vector{\theta})$ is the precision matrix. We have already assigned Gaussian priors for $\beta_x$ and $\epsilon_{x,t}$ (and then implicitly for $\eta_{x,t}$) and since the increments in the random walk priors for $\alpha_x$, $\kappa_t$ and $\gamma_k$ are assumed Gaussian, the assumption holds for these effects as well. 
\newline
We see that the first criterion is fulfilled, the Lee-Carter models can be written as LGMs. We also see from Expressions \ref{eq:LC-model} and \ref{eq:LCC-model-orig} that each $y_i$ only depends on the latent random field $\Vector{x}$ through $\eta_i$, so that the second criterion is also fulfilled. This means that the non-linearity of the predictior $\eta_{x,t}$, as discussed in Section \ref{sec:InlaRestrictions} is the only part of the Lee-Carter models that is not compatible with the \inla method. The methodology of the \inlabru framework (see Section \ref{sec:inlabru}) proposes a solution to this, by linearizing non-linear predictors in order to successfully run \inla. In the following sections, we apply the \inlabru method to the rewritten Lee-Carter models and assess the results of the inference. 

\newpage
\subsection{Generating Synthetic Data}
\label{sec:generating_synthetic_data}
\textcolor{myDarkGreen}{This is a rough draft}
We generate a synthetic set of data by running \inlabru on the German lung cancer data set, for the male population. We find the values of the hyperparameters $\tau_\alpha$, $\tau_\beta$, $\tau_\kappa$ and $\tau_\eps$ from these results, as well as the estimated intercept $\mu$. Using these values of the precisions, we sample values of the random effects $\alpha_x$, $\beta_x$, $\kappa_t$ and $\epsilon_{x,t}$. We sample the values of $beta_x$ and $\epsilon_{x,t}$ using a gaussian iid distribution, and we use random walk models of order one and two to sample the values of $\alpha_x$ and $\kappa_t$, respectively. We then shift the values of $\alpha_x$, $\kappa_t$ and $\beta_x$ so that they adhere to their respective sum-to-zero and sum-to-unit constraints. We obtain the value of the predictor $\eta_{x,t}$ by combining the random effects by
\begin{equation*}
    \eta_{x,t} = \mu + \lapha_x + \beta_x\cdot \kappa_t + \eps_{x,y}.
\end{equation*}
We then use the observed population at risk $E_{x,t}$ together with the synthetic values of the predictor $\eta_{x,t}$ to simulate Poisson-distributed values of the death counts $Y_{x,t}$:
\begin{equation}
    Y_{x,t} \sim \Poisson(E_{x,t} \cdot e^{\eta_{x,t}}).
\end{equation}
By doing this, we have obtained some synthetic observed death counts $Y_{x,t}$ which has the same dimensions of our real data, $x = 1,\ldots, 18$, $t = 1,\ldots, 18$ and which is of roughly the same order of magnitude, and for which we know the true values of $\eta_{x,t}$ and its decomposition into age- and period effects. We refer to this set of data as the \vFour data. 

We also do the same procedure to make a synthetic set of data from the male lung cancer data, where the data points from all ages under 45 years old are removed. We refer to this as the \vSeven data, and it has dimensions $x=1,\ldots,9$, $t = 1,\ldots,18$. Since we base the \vSeven data on the lung cancer data for older ages, for which there is a higher occurrance of lung cancer, the \vSeven data has an overall higher mortality level and the data $Y_{x,t}^{\vSevenNS}$ has fewer (\textcolor{myDarkGreen}{No?}) zero counts. 

\import{}{Results/SyntheticStanInlabru}





