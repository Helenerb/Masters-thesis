\newpage
\section{Literature Review - by Paper}
\label{sec_literature_review_by_paper}

\subsection{\textcite{LeeCarter1992}}The original paper of \textcite{LeeCarter1992}. 

\subsection{\textcite{HUNT_Villegas_2015}: Robustness and Convergence in the Lee-Carter Model with Cohort Effects}
Describes identifiability issues with cohort-extended Lee-Carter models. Connects this to the common two-step estimation procedure. Discusses lack of robustness in the full cohort-extended models in single-step estimation procedures (the closest procedure to our Bayesian model?). 

\textcite{Renshaw_Haberman_2006} have been under criticism; it converges slowly and is not robust to changes in the data. \textcite{Cairns_2009} suggests that this is due to an unresolved identifiability issue. \textcite{Renshaw_Haberman_2011} suggests that the issues observed by \textcite{Cairns_2009} originates from their fitting procedure. 

\textcite{HUNT_Villegas_2015}: Compares the two models, describes the identifiability mathematically, proposes a solution and quantifies the effect of the identifiability issues (?). 

\textcite{HUNT_Villegas_2015} states that the cohort-extended Lee-Carter model is endowed with an identifiability issue, and that this identifiability issue can be resolved by imposing an additional constraint on the parameters of the model. 

They inlcude a mathematical formulation of the identifiability in the normal LC-model, and how constraints on $\kappa_t$ and $\beta_x$ resolves this. 

They define a model M, which is one formulation of the cohort-extended Lee-Carter model:
\begin{equation*}
    \text{M:}\quad \log(\mu_{x,t}) = \alpha_x + \beta_x^{(1)}\kappa_t + \beta_x^{(0)}\iota_{t - x},
\end{equation*}
where $\iota_{t-x}$ is the cohort effects, modulated by the age effect $\beta_x^{(0)}$.  
In addition, they refer to the reduced model proposed by \textcite{Renshaw_Haberman_2006}
\begin{equation*}
    \text{H1: }\quad \log(\mu_{x,t}) = \alpha_x + \beta_x^{(1)}\kappa_t + \iota_{t - x}
\end{equation*}

Says this about the APC model:
" The second sub-model is the so-called age-period-cohort (APC) model, which has a long history in the fields of medicine and sociology (see e.g. \textcite{Clayton1987} and \textcite{Hobcraft_1982}), but it has not been widely used in the actuarial field until considered by \textcite{Currie_2006}. 

\subsubsection{On the two-stage fitting approach}
\textcite{Renshaw_Haberman_2006} propose to partition model M as
\begin{equation*}
    \mu_{x,t} = \exp(\alpha_x)F(x, t), \quad \log(F(x,t)) = \beta_x^{(1)}\kappa_t + \beta_x^{(0)}\iota_{t-x},
\end{equation*}
where $F(x,t)$ is then a set of reduction factors. With this partition, the two-stage fitting approach is
\begin{enumerate}
    \item $\alpha_x$ is found by $\alpha_x = \frac{1}{n}\sum_t\log(\frac{d_{xt}}{e_{xt}})$
    \item keeping $\alpha_x$ fixed, the other parameters are found by an iterative approach, as described by \textcite{Renshaw_Haberman_2006}, to maximize the log-likelihood
\end{enumerate}
A problem with this approach, as pointed out by \textcite{HUNT_Villegas_2015}, is that it will generally lead to poorer model fits than a one-stage approach where all parameters are fitted simultaneously. Since the parameters fitted in the second step of this procedure is conditioned on value of $\alpha_x$, one does not necessarily find the true maximum likelihood estimates. This approach will also lead to bias in the estimation of parameter uncertainty, since $\alpha_x$ is treated as known and all uncertainty is allocated to the other parameters. 

\textcite{Renshaw_Haberman_2009} argues that this reduction in goodness of fit can be justified by increased robustness to changes in data and a more easily explained model, in terms of demographic significance. 

The choices of model constraints are also discussed quite thoroughly in this paper. \textcite{Renshaw_Haberman_2006} impose the following parameter constraints on model M:
\begin{equation*}
    \sum_x\beta_x^{(1)} = \sum_x\beta_x^{(0)} = 1, \kappa_{t_1} = 0.
\end{equation*}
\textcite{HUNT_Villegas_2015} argues that the last constraint on $\kappa_{t_1}$ is excessive, and not really an identifiability constraint when the model is fitted with the two-stage approach. They argue that under the two-stage fitting approach, when $\alpha_x$ is fixed to $\alpha_x = \frac{1}{n}\sum_t d_{x,t}/e_{x,t}$ in the first stage, the transforms which require a constraint on the period effect are no longer valid. More specifically, with this fixation of $\alpha_s$, the relation $\sum_t\kappa_t \approx 0$ should follow naturally from the estimations. The constraint $\kappa_{t_1} = 0$ is then an additional constraint on the model, not just an identifiability constraint, and will alter the fit of the data instead of simply fixing the parameter estimates to one out of many equivalent realizations. \textcite{HUNT_Villegas_2015} states that this seems to be a recurring problem for this kind of two-stage estimation. 

A straightforward solution to this problem is to keep the two-stage approach, but remove the superfluous constraint on the period effect, here $\kappa_{t_1} = 0$. However, \textcite{HUNT_Villegas_2015} demonstrates that this approach gives estimates for the parameter values that are very sensitive to changes in the data. They show that this is the case for both models M and H1. 

They then consider a single-stage approach to fitting the models, suggested by \textcite{Cairns_2009}, where all parameters are fitted simultaneously using a Poisson likelihood maximization method. Under this fitting procedure, the model requires additional constraints. \textcite{Cairns_2009} choose 
\begin{equation*}
    \begin{aligned}
    &\sum_{x}\beta_x^{(1)} = \sum_{x}\beta_x^{(0)} = 1 \\
    &\sum_{t = t_1}^{t_n}\kappa_t = 0, \quad \sum_{t = t_1}^{t_n}\sum_{x = x_1}^{x_k}\iota_{t - x} = 0
    \end{aligned}
\end{equation*}
\textcite{HUNT_Villegas_2015} use a variation of the constraint on the cohort effect:
\begin{equation*}
    \sum_{c = t_1 - x_k}^{t_n - x_1} \iota_c = 0.
\end{equation*}
As mentioned also by \textcite{Cairns_2009}, the downside of this approach is that it can be very slow in converging to the solution, and it lacks robustness to changes in the data. \textcite{HUNT_Villegas_2015} continues with a thorough discussing and investigation of these robustness issues. 

A general concern is that the model is not able to find a global minimum, a suspicion that rises from the results showing that the different model fits does not have the same log-likelihood. 

\textcite{HUNT_Villegas_2015} also checks the robustness of the model when different parts of the data for the younger population is removed, since this might be a useful way to format the data. They find that the model H1 is much more robust than the M model when it comes to changes in the range of ages in the data. 

They then discuss the apparent identifiability issues leading to this lack of robustness. They discuss that some of the observed sensitivity in the parameter estimates, seemingly due to different initial values are a result of some additional identifiability. This additional identifiability is described mathematically, and it explains the observed tilting around the midpoint observed in some of the parameter estimates. 

\textcite{HUNT_Villegas_2015} suggest imposing an extra "apprioximate" identifiability constraint, removing linear drift from the cohort term:
\begin{equation*}
    \sum_{c = t_1 - x_k}^{t_n - x_1}(c - \hat{c})\iota_c = 0
\end{equation*}
They demonstrate that this constraint does change the model fit, but only slightly, and it does seem to remove quite a lot of the convergence issues. It does not remove all of the robustness issues, but \textcite{HUNT_Villegas_2015} concludes that the H1 model seems much more robust than the M model. 

Further, \textcite{HUNT_Villegas_2015} discusses whether the robustness issues that is still observed might be that it is in general difficult to find unique parameter estimates for this data. 

\subsection{\textcite{BEUTNER_2017}}
Describes identifiability issues in cohort-extended Lee-Carter models, attempts to prove that there is identifiability under some assumptions for so-called plug-in models.
  
\subsection{\textcite{fung_peters_shevchenko_2017}}
\textbm{A unified approach to mortality modelling using state-space framework: characterisation, identification, estimation and forecasting}
Describes Bayesian state space Lee-Carter models. Does not consider cohort extensions thoroughly in this paper, but refers to \textcite{fung_peters_shevchenko_2019} that discusses this. Focus on implementing volality of the error term (heteroscedaticity). Some interesting discussion on previous Bayesian vs frequentist (usually two-step) procedures, and also discusses downsides of the MCMC approach for Bayesian inference. 

\subsection{\textcite{Currie_2016}} 
Uses the gnm (Generalized Non-Linear Model) library in $\texttt{R}$ to fit Lee-Carter and Renshaw-Haberman mortality models, when they are formulated as gnms (which seems to be quite close to our formulation). They assume Poisson distributed deaths, with a log link function for the force of mortality. They do not include an error term in the linear predictor. The implementation in gnm seems quite similar to the inlabru implementation, but not Bayesian (at least no priors are given). They report of convergence issues with the Renshaw-Haberman (cohort) model. In Section 8, they discuss the challenges of forecasting with cohort-based models; argues that one should not forecast age, period and cohort effects independently as they are not independent.

\subsection{\textcite{hunt_blake_2020}}
Paper containing proof that age-period-cohort models on the form
\begin{equation}
    \eta_{x,t} = \alpha_x + \sum_{i=1}^{N}\beta_x^{(i)}\kappa_t^{(i)} + \gamma_{t-x}
\end{equation}
does not have any identifiability issues that is not also present in the corresponding age-period model, as long as the $\beta_x^{(i)}$ is non-parametric. Also discusses independence of projected period and cohort effects?
  
\subsection{\textcite{Hunt_blake_2021}}
First impression: interesting discussion of uncertainty in cohort parameters. Discusses properties of cohort effects, for instance that $\mathbb{E}[\gamma_c]=0$ and that the cohort effects should be treated as independent of the period effects - in contrast to the discussion of \textcite{Currie_2016}. Raises an important key issue; the different cohort effects are estimated based on different amounts of data. Fewer observations of younger cohorts. Argues that estimates of cohort effects based on historical data should not be treated as known with some parameter uncertainty, but as an initial estimate of an ongoing process. They argue that the cohort effect should have the following properties:
\begin{itemize}
    \item The cohort effects should embody genuine lifelong mortality effects, and should not be a misclassification of age- or period effects. \textcite{Hunt_blake_2021} have solved this through a two-step procedure where age and period effects are fitted first, we should not have to worry about it since we use an identifiable non-parametric model. 
    \item Cohort parameters should lack trends, such that $\mathbb{E}[\gamma_{t-x}] = 0$. \textcite{Hunt_blake_2021} ensure this by choosing identifiability constraints that eliminate polynomial trends of first and second order. Again, I don´t think we have to worry about this. Although, this desired property does support the choice of a drift-less random walk to model the cohort effect. 
    \item The cohort effects should be stationary; the variability from the expected zero mean should not change with time (birth year). This property seems to simply be inspired by the argument that there is no apparent reason for the cohort effect to be non-stationary.
    \item The projected cohort effects should be independent of the projected period effects. This is discussed more thoroughly in \textcite{hunt_blake_2020}, and is in opposition (I think) to what is argued by \textcite{Currie_2016}. 
    \item The cohort projection should take "unusual" cohorts into account, for instance those with birth year during the war. Unusual mortality rates for these cohorts seems to be an effect of poor quality of data rather than of actual cohort effects. I do not think this will be very relevant for our data. 
\end{itemize}
They do point one flaw of the single-step fit-and-project procedure, which is that it might be less suitable if we want to use the fitted model for different data sets where other time series models might be more appropriate? (I don´t really see this point, it is in footnote of page S240. How would we use the model from one data set to fit values for another data set? Isn´t this model quite data-specific?) 
\subsubsection{Data Generating Process for Cohorts}
\textit{This is a bit too technical - we don´t do this, so it should only be mentioned briefly, if mentioned at all}
To incorporate uncertainty about recent cohorts, \textcite{Hunt_blake_2021} specifies the underlying "data generating process" of the cohort mortality. They introduce the following quantities:
\begin{itemize}
    \item 
\end{itemize}

\subsection{\textcite{Wong_Forster_2018}  - Bayesian Mortality Froecasting with Overdispersion}
Discusses Bayesian inference with the Poisson Lee-Carter model, where an error term for overdispersion is added. Discusses the added ability to correctly estimate uncertainty in the estimation when this extra term is included. Uses MCMC for model fitting, goes through this procedure thoroughly. Does not include cohort effects, but discusses their presence in some of their results, and refers to future work on this topic. 

Presents two models accounting for overdispersion. The first is what they call the PLNLC model, the Poisson log-normal Lee-Carter model:
\begin{equation*}
    \begin{aligned}
        D_{x,t}\mid \mu_{x,t} \sim \Poisson(e_{x,t}\mu_{x,t}) \\
        \log(\mu_{x,t}) = \alpha_x + \beta_x\kappa_t + \nu_{x,t} \\
        \nu_{x,t}\mid \sigma_\mu^2 \sim \Normal(0, \sigma_\mu^2).
    \end{aligned}
\end{equation*}
The second model that they consider is the NBLC model, the Negative binomial Lee-Carter model:
\begin{equation*}
    \begin{aligned}
        D_{x,t}\mid \mu_{x,t} \sim \Poisson(e_{x,t}\mu_{x,t}) \\
        \log(\mu_{x,t}) = \alpha_x + \beta_x\kappa_t + \log(\nu_{x,t}) \\
         \nu_{x,t}\mid \phi \sim \gammaDist(\phi, \phi).
    \end{aligned}
\end{equation*}
This model structure was first proposed by \textcite{Delwarde_2007}. \textcite{Wong_Forster_2018} compares these two models to the Poisson Lee-Carter (PLC) model, proposed by \textcite{Czado_2005}, that does not account for overdispersion. Their results show that the PLNLC model and the NBLC model gives very similar performance in goodness of fit, and both perform significantly better than the PLC model. They recommend the NBLC model because of computational advantages. 

They give arguments for why a Bayesian approach is preferrable. 

\subsection{\textcite{LiKogure2021}: Bayesian Mixture Modelling for Mortality Projection}
Contains some arguments on why a Bayesian approach is desirable. Not published in a very respected journal, so perhaps don´t cite it too much.  

\subsection{\textcite{Renshaw_Haberman_2011}: A Comparative Study of Parametric Mortality Projection Models}
A well-written paper, discussing different mortality models. Considers the Lee-Carter model as a Parametric model, which I am a little confused about. But does not consider the models in a Bayesian setting, and does not discuss identifiability beyond applying constraints on the effects. Considers a log-odds link function, and considers the cohort-Lee-Carter structure with an age-modulated cohort effect.
\newline
Actually, they consider four versions of what you would consider the Lee-Carter model (note, with a different link-function than yourself).
\newline
Instead of applying the model to the force of mortality, or cases of deaths $Y_{x,t}$ directly, \textcite{Renshaw_Haberman_2011} defines the model in relation to the probability of death $q_{x,t}$. They define the model on the log-odds of the probability of death:
\begin{equation*}
    \log\big(\frac{q_{x,t}}{1 - q_{x,t}}\big) = \eta_{x,t}.
\end{equation*}
They consider a range of precictor structures, and among these are four structures which can be considered as versions of the Lee-Carter model, with or without a cohort extension:
\begin{equation*}
    \begin{aligned}
    LC:\quad  \eta_{x,t} &= \alpha_x + \beta_x\kappa_t\\ 
    H_1:\quad  \eta_{x,t} &= \alpha_x + \beta_x\kappa_t + \iota_{t - x}\\
    M:\quad  \eta_{x,t} &= \alpha_x + \beta_x\kappa_t + \beta_x^{(0)}\iota_{t-x}\\
    LC2:\quad \eta_{x,t} &= \alpha_x + \beta_x^{(1)}\kappa_t^{(1)} + \beta_x^{(2)}\kappa_t^{(2)}
    \end{aligned}
\end{equation*}
They consider these in relation to the basic age-period-effect model:
\begin{equation*}
    H_0:\quad \eta_{x,t} = \alpha_x + \kappa_t + \iota_{t-x}.
\end{equation*}
There are some interesting things to note about this. \textcolor{myDarkGreen}{I am not quite sure if the LC2 model is meant to capture some cohort effects. } \textcite{Renshaw_Haberman_2011} conclude that the LC and the LC2 model give similar predictions for the probability of mortality. Secondly, because of the relationship
\begin{equation*}
    \text{cohort = period - age},
\end{equation*}
they apply a two-stage fitting strategy. \textcolor{myDarkGreen}{Sånn jeg tolker dette: two-stage fitting procedure er nødvendig PÅ GRUNN AV identifiability mellom age, period og cohort. Dette er interessant. }

Does not consider ages below 20, arguing that they are considering the model in an actuarial setting. Refers to \textcite{LeeCarter1992} and Renshaw and Haberman(2003a,b, 2006).. Give some interesting results: 
\begin{itemize}
    \item Their Lee-Carter cohort and theie Lee-Carter model produce the same predictions as the Lee-Carter cohort model.. (Note that this containts an age-modulating effect on the cohort effect). Note also that this does not at all seem to be the case for your data... So you can talk about differences in disease data and actuarial data?
    \item 
    
\end{itemize}
\textcolor{myDarkGreen}{MERK: Noe du lurer veldig på. Renshaw og Haberman refererer til disse som parametriske; spesifikt referer de til prediktorer på formen f.eks. $\eta_{x,t} = \alpha_x + \beta_x\kappa_t$} som parametriske. Hva innebærer dette? Er parametrisk i samme betydning som Hunt og Blake refererer til det i tilfellet fet $\beta_x$ må være non-parametric for at du skal ha identifiability?

\section{TODO: Cairns et al, 2008x3, 2009}
Renshaw and Haberman refererer mye til Cairns, de har modellene der $\beta_x = (x - \bar{x})$ osv. Ligner ikke så mye på det vi skal ha, men sikkert greit å nevne... 